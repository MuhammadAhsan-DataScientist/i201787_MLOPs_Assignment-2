{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3cab0bc-cec4-4cb1-b5c0-a221829b3d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Data has been successfully written to: scrapped_bbc_articles.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import json\n",
    "\n",
    "# Base URLs of news sites\n",
    "BASE_BBC_URL = \"https://www.bbc.com\"\n",
    "BASE_DAWN_URL = \"https://www.dawn.com\"\n",
    "\n",
    "\n",
    "articles = []\n",
    "\n",
    "# Function to get article links\n",
    "def extract_article_links(base_url):\n",
    "    response = requests.get(base_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    article_links = [urljoin(base_url, link['href']) for link in soup.find_all('a', href=True)]\n",
    "    return article_links\n",
    "\n",
    "# Function to get article content\n",
    "def extract_article_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    title = soup.find('h1').text if soup.find('h1') else ''\n",
    "    description = soup.find('meta', {'name': 'description'})['content'] if soup.find('meta', {'name': 'description'}) else ''\n",
    "    return title, description\n",
    "\n",
    "# Fetch article links and their content from BBC\n",
    "links = extract_article_links(BASE_BBC_URL)\n",
    "\n",
    "for link in links:\n",
    "    try:\n",
    "        title, description = extract_article_content(link)\n",
    "        # Add each article to the articles list\n",
    "        articles.append({\"title\": title, \"description\": description})\n",
    "    except Exception as e:\n",
    "        print(f\"Error with URL {link}: {e}\")\n",
    "\n",
    "# Save articles\n",
    "output_path = 'scrapped_bbc_articles.json'\n",
    "with open(output_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(articles, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"The Data has been successfully written to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fa723ff-3919-42b0-945b-74dcd141379a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data successfully written to bbc_articles_cleaned.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Muneeba\\AppData\\Local\\Temp\\ipykernel_10936\\3249117024.py:7: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  text = re.sub('\\s+', ' ', text).strip()  # Remove extra spaces and newlines\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Function to clean text\n",
    "def cleaned_text(text):\n",
    "    text = re.sub('<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub('\\s+', ' ', text).strip()  # Remove extra spaces and newlines\n",
    "    return text\n",
    "\n",
    "# Load data from the existing JSON file\n",
    "input_path = 'scrapped_bbc_articles.json'\n",
    "with open(input_path, 'r', encoding='utf-8') as json_file:\n",
    "    articles = json.load(json_file)\n",
    "\n",
    "# Clean each article's title and description\n",
    "for article in articles:\n",
    "    article['title'] = cleaned_text(article.get('title', ''))\n",
    "    article['description'] = cleaned_text(article.get('description', ''))\n",
    "\n",
    "# Save cleaned data\n",
    "output_path = 'bbc_articles_cleaned.json'\n",
    "with open(output_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(articles, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Cleaned data successfully written to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94190395-a2f0-4155-a412-393366106975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?client_id=461645083159-nq6vu5pdur3ua86lldafjik3msc9cetp.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=offline&response_type=code\n",
      "\n",
      "Authentication successful.\n"
     ]
    }
   ],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "\n",
    "# Set the path to your client secrets file\n",
    "client_secrets_path = 'client_secrets.json'\n",
    "\n",
    "# Authenticate with Google Drive\n",
    "gauth = GoogleAuth()\n",
    "gauth.LoadClientConfigFile(client_secrets_path)\n",
    "gauth.LocalWebserverAuth()\n",
    "\n",
    "# Initialize Google Drive client\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "# Example function to upload a file\n",
    "def upload_file_to_drive(file_path, drive_folder_id):\n",
    "    file = drive.CreateFile({'parents': [{'id': drive_folder_id}]})\n",
    "    file.SetContentFile(file_path)\n",
    "    file.Upload()\n",
    "\n",
    "\n",
    "drive_folder_id = \"1ykMTaoSCHoW7QzFWmzmlaxd7IcCaqftD\"\n",
    "file_path = r\"C:\\Users\\Ahsan\\Downloads\\i201787_Assignment#2\\bbc_articles_cleaned.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caff29a2-1d5b-4e9a-9ffb-4b057339444b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
